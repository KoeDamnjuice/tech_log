{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07472dcd-b027-41f3-a1d2-c6982e259d06",
   "metadata": {},
   "source": [
    " # 何为BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f603297",
   "metadata": {},
   "source": [
    "\n",
    " BPE本质是NLP技术的一种文本编码压缩技术，特别适合处理未见词问题。  \n",
    " BPE的核心在于迭代地统计文本中最频繁出现的字符或字符对，并将其合并为字典中的新符号。  \n",
    "\n",
    " ***BPE的一般迭代步骤：***\n",
    " 1. 初始化词汇表，比如期望的词汇表长度，并把语言基本字符灌入词汇表\n",
    " 2. 统计所有相邻字符对的频率\n",
    " 3. 将频率最高的一批字符对合并为新单元，更新到词汇表中\n",
    " 4. 重复以上步骤，知道达到终止条件：比如最大词汇数、最大合并次数等等。\n",
    "\n",
    "***英文BPE与中文BPE的区别***  \n",
    "英文BPE的基本字符是字母，合并的新词可能是字母对、单词之列的。\n",
    "而中文的基本字符是字，合并出来的结果是词语。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51133568",
   "metadata": {},
   "source": [
    "NLP技术的一种文本编码压缩技术，特别适合处理未见词问题。  \n",
    " BPE的核心在于迭代地统计文本中最频繁出现的字符或字符对，并将其合并为字典中的新符号。  \n",
    "\n",
    " ***BPE的一般迭代步骤：***\n",
    " 1. 初始化词汇表，比如期望的词汇表长度，并把语言基本字符灌入词汇表\n",
    " 2. 统计所有相邻字符对的频率\n",
    " 3. 将频率最高的一批字符对合并为新单元，更新到词汇表中\n",
    " 4. 重复以上步骤，知道达到终止条件：比如最大词汇数、最大合并次数等等。\n",
    "\n",
    "***英文BPE与中文BPE的区别***  \n",
    "英文BPE的基本字符是字母，合并的新词可能是字母对、单词之列的。\n",
    "而中文的基本字符是字，合并出来的结果是词语。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec156b77-1b41-4f49-8eb2-bbc4008a9aec",
   "metadata": {},
   "source": [
    "## BPE example\n",
    "\n",
    "### 初始数据\n",
    "假设我们有一个小型的语料库，只包含以下句子：  \n",
    "+ \"low\"\n",
    "+ \"lower\"\n",
    "+ \"newest\"\n",
    "+ \"widest\"  \n",
    "首先，我们需要对每个词进行基础的字符分割，并添加一个特殊字符</w>来表示词尾。这样可以帮助模型学习单词的边界，防止将单词末尾的字符与其他单词开头的字符混淆。我们的初始数据看起来会是这样：  \n",
    "+ \"l o w </w>\"\n",
    "+ \"l o w e r </w>\"\n",
    "+ \"n e w e s t </w>\"\n",
    "+ \"w i d e s t </w>\"  \n",
    "### 统计字符对的频率  \n",
    "接下来，我们统计所有相邻字符对的频率。例如，“l o”出现了两次（在\"low\"和\"lower\"中），\"e s\"出现了两次（在\"newest\"和\"widest\"中），等等。  \n",
    "\n",
    "### 合并最频繁的字符对\n",
    "假设最频繁的字符对是“e s”，我们将它们合并为一个新单元“es”。更新文本表示如下：  \n",
    "\n",
    "+ \"l o w </w>\"\n",
    "+ \"l o w e r </w>\"\n",
    "+ \"n e w es t </w>\" \n",
    "+ \"w i d es t </w>\"  \n",
    "### 重复合并过程\n",
    "接着，我们继续统计并合并最频繁的字符对。假设下一个最频繁的字符对是“es t”，我们再次合并：  \n",
    "\n",
    "+ \"l o w </w>\"\n",
    "+ \"l o w e r </w>\"\n",
    "+ \"n e w est </w>\"\n",
    "+ \"w i d est </w>\"  \n",
    "继续这个过程，可能下一步是合并“l o”为“lo”：\n",
    "\n",
    "\"lo w </w>\"\n",
    "\"lo w e r </w>\"\n",
    "\"n e w est </w>\"\n",
    "\"w i d est </w>\"  \n",
    "### 结果\n",
    "通过不断重复这个过程，我们逐步构建了一个能够识别新词和词尾的词汇表，同时将频繁出现的字符或字符组合编码为单个单元。最终，我们的词汇表可能包括：“low”, “lower”, “newest”, “widest”，以及一系列合并形成的子词单元如“lo”, “w”, “es”, “est”等。  \n",
    "\n",
    "这种方法的优势在于，即使是未见过的词，只要它包含了足够的已知子词单元，模型也有可能正确处理和理解它，从而显著提高了模型对未知词汇的适应能力。这使得BPE在NLP任务中特别有用，如机器翻译、文本生成等。\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41abfc-974a-4798-beeb-07410c94f838",
   "metadata": {},
   "source": [
    "# 旋转位置编码RoPE是什么\n",
    "RoPE 的核心思想是通过将**位置信息**编码为**向量的旋转**，来使模型能够更自然地处理序列中元素间的相对位置关系。  \n",
    "即，RoPE把序列中的位置转换为了一个在高维空间中旋转的向量。\n",
    "\n",
    "+ 首先把每个位置映射为一个标准旋转向量，角度与位置成比例。\n",
    "+ 将标准旋转向量与输入嵌入相乘。\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6c4f7-4be8-483d-90f3-153c746d4d72",
   "metadata": {},
   "source": [
    "# Encoder-only, Decoder-only, Encoder-Decoder大模型是什么\n",
    "**Encoder-only**  \n",
    "结构上就是transformer的编码层，作用是对输入的序列信息提取特征，输出高度抽象的嵌入表示，以服务后续工作，比如文本分类、情感分类。\n",
    "案例： BERT\n",
    "\n",
    "**Decoder-only**  \n",
    "是一种生成式的模型，作用为根据给定的上下文，生成文本或者图片描述。  \n",
    "案例: GPT\n",
    "\n",
    "**Encoder-Decoder**\n",
    "是前二者的结合，把一种输入转换为另一种输出，适合机器翻译、ASR等工作。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60065d",
   "metadata": {},
   "source": [
    "# 大模型中的层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476b173",
   "metadata": {},
   "source": [
    "**层归一化**的含义是，对于某个数据样本，对其某神经网络层的输出直接进行归一化，这避免了batchnorm对batch量的要求。\n",
    "对transformer的每层，实际上有两种应用层归一化的方法：  \n",
    "+ 前归一化：在每一层注意力机制和前馈神经网络之前对输入数据进行层归一化（但是残差连接数据并不会进行归一化处理）\n",
    "    - 这可以更好地加速模型训练收敛，并降低梯度消失的风险（因为层归一化使得输入数据分布稳定，从而使得梯度的变化也更为稳定）\n",
    "    - 该方法广泛应用到BERT中，被证明拥有更快的收敛速度和稳定性。\n",
    "    - 但由于前归一化对输入数据进行了归一化，导致某些数据特征被弱化，从而降低了模型的性能。\n",
    "+ 后归一化 ：是原始transformer架构使用的层归一化方式，这会导致第一层transformer的输入数据是原始数据\n",
    "    - 由于后归一化不会对原始输入数据进行归一化处理，因此可以让神经网络层捕捉到原始数据中的微弱细节，提升模型的性能。\n",
    "    - 相比前归一化，更容易出现训练的不稳定（消失爆炸），且收敛相对缓慢。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b4a10",
   "metadata": {},
   "source": [
    "# Transformer中使用的激活函数gelu\n",
    "![](gelu1.png)  \n",
    "如上图所示，是标准正态分布的分布函数(CDF),将他与x相乘，就得到了transformer中广泛使用的gelu分布函数。  \n",
    "![](gelu2.png)  \n",
    "gelu相对于relu更加平滑，在小于零的位置梯度不会马上归零。同时保有了relu和sigmoid的特性，而且他是过零点的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe10e2c",
   "metadata": {},
   "source": [
    "# transformer的多头注意力\n",
    "多头注意力是transformer的一种机制，使得模型能从不同的特征子空间关注信息，从而使得模型能从多个角度审视信息，增强模型处理复杂信息的能力。  \n",
    "核心概念：  \n",
    "+ 多头并行：将注意力机制拆解成多个头，每个头独立处理输入数据集。\n",
    "+ QKV三个矩阵独立运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283841d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf92f0f7",
   "metadata": {},
   "source": [
    "# 大模型分布式训练\n",
    "使用分布式训练，是因为现代大模型的数据量已经非常大，单独的显卡根本无法胜任对其训练的目标。\n",
    "## NCCL (NVIDIA Collective Communications Library)英伟达集群通信库  \n",
    "在这个库中有一些列常用于分布式训练中，显卡常用的集群通信接口。\n",
    "+ Broadcast：将张量从主gpu广播到其他gpu\n",
    "+ scatter：将主gpu的数据分块，分发到其他gpu\n",
    "+ reduce：对所有gpu的数据进行一种规约处理（求和、乘积、平均等等），并发回主gpu\n",
    "+ all-reduce：进行reduce以后，把主gpu的数据广播到所有gpu\n",
    "+ reduce-scatter：进行reduce以后，把主gpu的数据scatter到各个gpu  \n",
    "\n",
    "## 数据并行分布式训练  \n",
    "数据并行的概念图如下：  \n",
    "![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f2db22e52a3d45b2804d09d06dd2eca6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=793&h=720&s=2288325&e=png&b=fefefe)  \n",
    "基本原理是把batch数据再次划分，并把模型完整复制到各个gpu，主要方法分为dp和ddp\n",
    "### 远古版本：data parallelism(DP)数据并行\n",
    "DP是torch中最原始的分布式训练方式，属于单进程多线程训练。首先将模型完整复制到N个显卡，然后把输入数据分为N份，在各个显卡上进行前向传播。然后，各个显卡把输出发送给主GPU，主GPU计算总的loss，并把损失广播给各个gpu，各个gpu计算梯度，并把各自计算的梯度gather到主gpu，gpu通过梯度进行参数更新，并把新的参数广播到各个gpu。如果模型总的参数量是$\\tau$的话，那么主显卡的输入输出通讯录是$(N-1)\\tau$, 而其他显卡的通信量是$\\tau$\n",
    "\n",
    "### 分布式数据并行(distributed data parallelism)\n",
    "DDP属于真正的多进程训练，每个进程都有独立的优化器与python解释器，并计算梯度。同时，通过集群通信的ring-all-reduce。DDP的多卡负载更加均衡。在ring-all-reduce环节中，所有显卡的通信量都是$2\\tau$\n",
    "\n",
    "### DP与DDP的区别\n",
    "+ DP只适合单机多卡的场景，DDP适合单机多卡、多机多卡。由于DDP属于多进程实现，因此可以避免python解释器的GIL锁带来的影响。\n",
    "+ 由于DP的梯度计算和梯度更新都在主卡上进行，因此主卡负载明显高于其他卡。而DDP的负载相对均衡，训练更高效，这使得DP被弃用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d2923",
   "metadata": {},
   "source": [
    "## Zero零冗余优化器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb99f8",
   "metadata": {},
   "source": [
    "\n",
    "zero技术是微软提出用于消除训练中显存冗余的一种算法，有三个版本，分别为os、os+g、os+g+p。在传统数据并行混合精度训练中，每个gpu中不仅要存储fp16的模型参数、fp16的梯度，以及优化器中fp32的参数、梯度、adam一阶momentum、二阶varianace。如果模型参数是$\\tau$,一个fp16占2bytes，fp32占4bytes，那么显存总占用为$2\\tau+2\\tau+4\\tau+4\\tau+4\\tau+4\\tau=20\\tau$。其中优化器状态占用非常大，且各个gpu的优化器状态是冗余的。  \n",
    "\n",
    "### 普通的分布式训练过程如下：   \n",
    "下图为普通的混合精度分布式训练运行过程：  \n",
    "![](Zero_0.png)  \n",
    "1. 将数据并行，分别传给gpu0、1、2\n",
    "2. 随后根据fp16的参数，进行前向传播与反向传播，计算得到fp16的梯度\n",
    "3. 将梯度精度放大到fp32，并根据此计算fp32的一阶动量、二阶方差，并更新fp32的模型参数\n",
    "4. 将fp32的模型参数精度缩小到fp16并更新参数。\n",
    "\n",
    "### zero_1优化器的训练过程如下：    \n",
    "下图为使用zero os优化器进行混合精度分布式训练的过程：\n",
    "![](Zero_1.png)  \n",
    "1. 将数据并行，分别传给gpu0、1、2\n",
    "2. 随后根据fp16的参数，进行前向传播\n",
    "3. gpu0、1、2总数为3，他们各负责更新$\\tau/3$的参数, 因此，他们也只需要保留三分之一的adam优化器状态在显存中就行了。  \n",
    "4. 三个gpu分别进行反向传播，通过all-reduce等方法，同步三个gpu计算的平均值。\n",
    "5. 三个gpu分别把子集对应部分的梯度传给分片的adam优化器，更新fp16参数, 并将fp16参数进行广播到其他cpu\n",
    "6. 这只方法的通讯量与DDP相同，都是$2\\tau$, 但是显存占用率远低于标准DDP。 \n",
    "\n",
    "### zero_2优化器的训练过程如下：\n",
    "zero os+g在os的基础上，对fp16的梯度也进行了划分，如下：  \n",
    "![](Zero_2.png)  \n",
    "1. 在os+g上，每个gpu只保留所需维护参数部分的fp16梯度\n",
    "2. 在反向传播时，如果该gpu不需要维护这段参数，那么计算完梯度后立即与所有gpu计算的梯度平均，并释放这段梯度。而需要维护这段参数的gpu，则保留这段梯度，用于参数更新。\n",
    "\n",
    "### zero_3：\n",
    "zero os+g+p，甚至把参数也划分了。  \n",
    "![](Zero_3.png)  \n",
    "1. 在前向传播时，各个gpu只保留了需要维护参数段的参数，因此，存有参数的gpu需要把该段的参数广播给其他gpu，进行前向传播，传播结束之后立即释放参数显存。\n",
    "2. 反向传播也需要参数，因此同样持有该参数的gpu需要在反向传播时广播参数，不持有参数的gpu通过参数计算得到梯度之后，立即释放参数显存，并计算梯度均值。\n",
    "3. 由于zero3前向传播和后向传播都需要进行广播参数，因此通信量是大于zero1和2的，显存占用却是最低。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742fbf1",
   "metadata": {},
   "source": [
    "### 完全分片数据并行FSDP\n",
    "对比于DDP，FSDP将模型及其参数、梯度、训练器参数分片放到各个GPU上，当前向传播与反向传播时，先将各个gpu的参数分片进行all gather，使得各个gpu有完整的参数，计算得到梯度，在进行一次reduce-scatter,使得各个gpu再次只保存一部分梯度和参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482275c",
   "metadata": {},
   "source": [
    "# 模型并行\n",
    "\n",
    "## 流水线并行(pipeline parallelism)\n",
    "### Naive朴素流水线并行\n",
    "朴素流水线并行把模型按层分割到多个显卡之中，流水线式的进行前向传播，然后进行反向传播。这会导致一个显卡运行计算时，其他显卡都处于空闲状态，这会导致大量的资源浪费。  \n",
    "![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e083cc0698e84ff0b452c71d67699d3c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)  \n",
    "\n",
    "### mini-batch pipeline parallelism(小批次流水线并行)\n",
    "把mini-batch拆分成多个micro-batch，使得一个gpu处理完一批数据传递给下一层数据后，还会继续处理下一个micro-batch而不会进入空载状态，这进一步减少了显卡的等待与阻塞时间。\n",
    "![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/47ea6c4f10884a15adaee2801a9e1369~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)  \n",
    "mini-batch pipeline parallelism的实现：  \n",
    "+ Gpipeline：谷歌发行，首先使用mini-batch流水线并行来降低流水线并行的bubble时间。此外，显存中不保存激活值，只保存多层模型的输出值，在反向传播时，重新计算re-materialization激活值，从而得到最后梯度，以降低显卡显存峰值。  \n",
    "+ pipeDream: 在mini batch流水线并行基础上,又加入了1F1B机制,使得激活值能够被即时释放,虽然在bubble时间上相对Gpipeline没有优化,但优化了显存占用.\n",
    "\n",
    "流水线并行的两种策略：\n",
    "+ F-then-B策略：在这种策略下，所有显卡计算完前向，才依次执行反向传播\n",
    "![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/97065fc03afe40e4a413a5fed0786f60~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)\n",
    "\n",
    "+ 1F1B策略:每张显卡依次进行前向和反向传播,这样可以使得前向的缓存数据可以即时得到释放(相比F then B方式,可以节省37.5%的显存). \n",
    "![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e5663065f12d495fade59454730aeba5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)\n",
    "\n",
    "## 张量并行\n",
    "张量并行与流水线并行不同,张量并行把模型中的张量参数分割到不同的gpu中, 使用线性代数分块矩阵计算的原理来进行模型并行.\n",
    "![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8a8fcd144b564483916f34ac801eadb3~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)  \n",
    "\n",
    "在transformer的每个子注意力层中,包含多头子注意力模块和前馈MLP模块.\n",
    "对MLP模块的张量并行比较简单,因为整体计算公式为XW1W2, 对W1进行按列分块,W2进行按行分块,就可以进行张量并行计算了.\n",
    "对多层自注意力机制模块,可以把一个或多个子注意力块放入一个显卡中,但注意最好让头数是显卡数的倍数.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131cd8b",
   "metadata": {},
   "source": [
    "# 混合精度训练\n",
    "## fp16与fp32\n",
    "在模型训练中,使用fp16进行前向反向传播可以提升计算速度和降低显存占用.但是,使用fp16计数,最大精度位2的负十次方,因为尾数位十位, 指数位5位,还有符号位1位.在神经网络训练后期,梯度往往会非常小,学习率也非常低,在参数更新的时候,会导致参数的更新量低于fp16的最大精度,导致参数无法更新.因此在优化器中,往往会存储模型参数的fp32副本,在进行反向传播计算梯度的时候,先回把计算得到的梯度放大,使得fp16可以存储,在进入优化器之前,就可以缩小并恢复为fp32,从而进行正确的参数更新.在adam优化器中,不仅要保存fp32的参数,还有fp32的一阶momentum和二阶variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845da112",
   "metadata": {},
   "source": [
    "# 参数高效微调(Parameter-Efficient Fine-Tuning, PEFT)策略:\n",
    "为什么要对LLM进行微调，因为要让预训练好的大模型去适应不同的下游任务。\n",
    "虽然类似GPT这样的大模型已经拥有强大的泛化能力，在一些情况下，直接在前向推理中使用in-context learning就可以让模型适应不同的下游任务，但是效果依然没有fine-tune的好。  \n",
    "然而，直接对模型进行fine-tune是不容易的，gpu中不仅要存储LLM的巨大参数量，还要存储对等参数的梯度、优化器的参数。  \n",
    "几种优化器需要存储的模型参数副本量如下：\n",
    "+ SGD: 0\n",
    "+ RMSProp: 1\n",
    "+ Adam: 2  \n",
    "现代LLM本身就带有了强力的多任务处理能力，说明LLM本身就含有处理不同任务的神经元，而我们只需要针对特定的神经元进行强化，就可以让LLM来适应不同的下游任务。  \n",
    "两大类PEFT方法：\n",
    "+ Adapater，为LLM添加些许模块，固定原有LLM参数，进行训练\n",
    "+ Prefixing， 为prompt加入一些前缀Special Token，让模型更好地适应多任务。比如whisper的special token：translate、transcirbe、语言种类。。。  \n",
    "## LoRA（低秩适配）方法\n",
    "LoRA属于Adapter类型，他为transformer的多头自注意力层、FF层添加一个插件块，这个插件块透过Rank-r限缩来降低参数量，这应用了数学的矩阵乘积分解与瓶颈层技术来降低参数量。新增的LoRA模块虽然与原始大模型参数shape相同，假设为d，但是通过矩阵分解为A(d * r)和B(r * d)，使得新增插件需要训练的参数量极低。适应LoRA方法微调LLM，需训练的参数量低于原始参数量0.1%。\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7FlF-3H6qvzgsRbS.png)  \n",
    "**结果：**LoRA方法微调模型，可以在可调参数量极低的情况下，性能超越绝大多数微调方法。  \n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*py72LfXhHyTjlopp.png)  \n",
    "**在attention层的不同位置插入LoRA模块的结果对比:**  \n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6SbzXVdQlhjLae1k.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bba18a",
   "metadata": {},
   "source": [
    "# transformer结构理解\n",
    "https://zhuanlan.zhihu.com/p/338817680"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
