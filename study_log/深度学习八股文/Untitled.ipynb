{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07472dcd-b027-41f3-a1d2-c6982e259d06",
   "metadata": {},
   "source": [
    " # 何为BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f603297",
   "metadata": {},
   "source": [
    "\n",
    " BPE本质是NLP技术的一种文本编码压缩技术，特别适合处理未见词问题。  \n",
    " BPE的核心在于迭代地统计文本中最频繁出现的字符或字符对，并将其合并为字典中的新符号。  \n",
    "\n",
    " ***BPE的一般迭代步骤：***\n",
    " 1. 初始化词汇表，比如期望的词汇表长度，并把语言基本字符灌入词汇表\n",
    " 2. 统计所有相邻字符对的频率\n",
    " 3. 将频率最高的一批字符对合并为新单元，更新到词汇表中\n",
    " 4. 重复以上步骤，知道达到终止条件：比如最大词汇数、最大合并次数等等。\n",
    "\n",
    "***英文BPE与中文BPE的区别***  \n",
    "英文BPE的基本字符是字母，合并的新词可能是字母对、单词之列的。\n",
    "而中文的基本字符是字，合并出来的结果是词语。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51133568",
   "metadata": {},
   "source": [
    "NLP技术的一种文本编码压缩技术，特别适合处理未见词问题。  \n",
    " BPE的核心在于迭代地统计文本中最频繁出现的字符或字符对，并将其合并为字典中的新符号。  \n",
    "\n",
    " ***BPE的一般迭代步骤：***\n",
    " 1. 初始化词汇表，比如期望的词汇表长度，并把语言基本字符灌入词汇表\n",
    " 2. 统计所有相邻字符对的频率\n",
    " 3. 将频率最高的一批字符对合并为新单元，更新到词汇表中\n",
    " 4. 重复以上步骤，知道达到终止条件：比如最大词汇数、最大合并次数等等。\n",
    "\n",
    "***英文BPE与中文BPE的区别***  \n",
    "英文BPE的基本字符是字母，合并的新词可能是字母对、单词之列的。\n",
    "而中文的基本字符是字，合并出来的结果是词语。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec156b77-1b41-4f49-8eb2-bbc4008a9aec",
   "metadata": {},
   "source": [
    "## BPE example\n",
    "\n",
    "### 初始数据\n",
    "假设我们有一个小型的语料库，只包含以下句子：  \n",
    "+ \"low\"\n",
    "+ \"lower\"\n",
    "+ \"newest\"\n",
    "+ \"widest\"  \n",
    "首先，我们需要对每个词进行基础的字符分割，并添加一个特殊字符</w>来表示词尾。这样可以帮助模型学习单词的边界，防止将单词末尾的字符与其他单词开头的字符混淆。我们的初始数据看起来会是这样：  \n",
    "+ \"l o w </w>\"\n",
    "+ \"l o w e r </w>\"\n",
    "+ \"n e w e s t </w>\"\n",
    "+ \"w i d e s t </w>\"  \n",
    "### 统计字符对的频率  \n",
    "接下来，我们统计所有相邻字符对的频率。例如，“l o”出现了两次（在\"low\"和\"lower\"中），\"e s\"出现了两次（在\"newest\"和\"widest\"中），等等。  \n",
    "\n",
    "### 合并最频繁的字符对\n",
    "假设最频繁的字符对是“e s”，我们将它们合并为一个新单元“es”。更新文本表示如下：  \n",
    "\n",
    "+ \"l o w </w>\"\n",
    "+ \"l o w e r </w>\"\n",
    "+ \"n e w es t </w>\" \n",
    "+ \"w i d es t </w>\"  \n",
    "### 重复合并过程\n",
    "接着，我们继续统计并合并最频繁的字符对。假设下一个最频繁的字符对是“es t”，我们再次合并：  \n",
    "\n",
    "+ \"l o w </w>\"\n",
    "+ \"l o w e r </w>\"\n",
    "+ \"n e w est </w>\"\n",
    "+ \"w i d est </w>\"  \n",
    "继续这个过程，可能下一步是合并“l o”为“lo”：\n",
    "\n",
    "\"lo w </w>\"\n",
    "\"lo w e r </w>\"\n",
    "\"n e w est </w>\"\n",
    "\"w i d est </w>\"  \n",
    "### 结果\n",
    "通过不断重复这个过程，我们逐步构建了一个能够识别新词和词尾的词汇表，同时将频繁出现的字符或字符组合编码为单个单元。最终，我们的词汇表可能包括：“low”, “lower”, “newest”, “widest”，以及一系列合并形成的子词单元如“lo”, “w”, “es”, “est”等。  \n",
    "\n",
    "这种方法的优势在于，即使是未见过的词，只要它包含了足够的已知子词单元，模型也有可能正确处理和理解它，从而显著提高了模型对未知词汇的适应能力。这使得BPE在NLP任务中特别有用，如机器翻译、文本生成等。\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41abfc-974a-4798-beeb-07410c94f838",
   "metadata": {},
   "source": [
    "# 旋转位置编码RoPE是什么\n",
    "RoPE 的核心思想是通过将**位置信息**编码为**向量的旋转**，来使模型能够更自然地处理序列中元素间的相对位置关系。  \n",
    "即，RoPE把序列中的位置转换为了一个在高维空间中旋转的向量。\n",
    "\n",
    "+ 首先把每个位置映射为一个标准旋转向量，角度与位置成比例。\n",
    "+ 将标准旋转向量与输入嵌入相乘。\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6c4f7-4be8-483d-90f3-153c746d4d72",
   "metadata": {},
   "source": [
    "# Encoder-only, Decoder-only, Encoder-Decoder大模型是什么\n",
    "**Encoder-only**  \n",
    "结构上就是transformer的编码层，作用是对输入的序列信息提取特征，输出高度抽象的嵌入表示，以服务后续工作，比如文本分类、情感分类。\n",
    "案例： BERT\n",
    "\n",
    "**Decoder-only**  \n",
    "是一种生成式的模型，作用为根据给定的上下文，生成文本或者图片描述。  \n",
    "案例: GPT\n",
    "\n",
    "**Encoder-Decoder**\n",
    "是前二者的结合，把一种输入转换为另一种输出，适合机器翻译、ASR等工作。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60065d",
   "metadata": {},
   "source": [
    "# 大模型中的层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476b173",
   "metadata": {},
   "source": [
    "**层归一化**的含义是，对于某个数据样本，对其某神经网络层的输出直接进行归一化，这避免了batchnorm对batch量的要求。\n",
    "对transformer的每层，实际上有两种应用层归一化的方法：  \n",
    "+ 前归一化：在每一层注意力机制和前馈神经网络之前对输入数据进行层归一化（但是残差连接数据并不会进行归一化处理）\n",
    "    - 这可以更好地加速模型训练收敛，并降低梯度消失的风险（因为层归一化使得输入数据分布稳定，从而使得梯度的变化也更为稳定）\n",
    "    - 该方法广泛应用到BERT中，被证明拥有更快的收敛速度和稳定性。\n",
    "    - 但由于前归一化对输入数据进行了归一化，导致某些数据特征被弱化，从而降低了模型的性能。\n",
    "+ 后归一化 ：是原始transformer架构使用的层归一化方式，这会导致第一层transformer的输入数据是原始数据\n",
    "    - 由于后归一化不会对原始输入数据进行归一化处理，因此可以让神经网络层捕捉到原始数据中的微弱细节，提升模型的性能。\n",
    "    - 相比前归一化，更容易出现训练的不稳定（消失爆炸），且收敛相对缓慢。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b4a10",
   "metadata": {},
   "source": [
    "# Transformer中使用的激活函数gelu\n",
    "![](gelu1.png)  \n",
    "如上图所示，是标准正态分布的分布函数(CDF),将他与x相乘，就得到了transformer中广泛使用的gelu分布函数。  \n",
    "![](gelu2.png)  \n",
    "gelu相对于relu更加平滑，在小于零的位置梯度不会马上归零。同时保有了relu和sigmoid的特性，而且他是过零点的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe10e2c",
   "metadata": {},
   "source": [
    "# transformer的多头注意力\n",
    "多头注意力是transformer的一种机制，使得模型能从不同的特征子空间关注信息，从而使得模型能从多个角度审视信息，增强模型处理复杂信息的能力。  \n",
    "核心概念：  \n",
    "+ 多头并行：将注意力机制拆解成多个头，每个头独立处理输入数据集。\n",
    "+ QKV三个矩阵独立运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283841d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf92f0f7",
   "metadata": {},
   "source": [
    "# 大模型分布式训练\n",
    "使用分布式训练，是因为现代大模型的数据量已经非常大，单独的显卡根本无法胜任对其训练的目标。\n",
    "## NCCL (NVIDIA Collective Communications Library)英伟达集群通信库  \n",
    "在这个库中有一些列常用于分布式训练中，显卡常用的集群通信接口。\n",
    "+ Broadcast：将张量从主gpu广播到其他gpu\n",
    "+ scatter：将主gpu的数据分块，分发到其他gpu\n",
    "+ reduce：对所有gpu的数据进行一种规约处理（求和、乘积、平均等等），并发回主gpu\n",
    "+ all-reduce：进行reduce以后，把主gpu的数据广播到所有gpu\n",
    "+ reduce-scatter：进行reduce以后，把主gpu的数据scatter到各个gpu  \n",
    "\n",
    "## 数据并行分布式训练  \n",
    "数据并行的概念图如下：  \n",
    "![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f2db22e52a3d45b2804d09d06dd2eca6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=793&h=720&s=2288325&e=png&b=fefefe)  \n",
    "基本原理是把batch数据再次划分，并把模型完整复制到各个gpu，主要方法分为dp和ddp\n",
    "### 远古版本：data parallelism(DP)数据并行\n",
    "DP是torch中最原始的分布式训练方式，属于单进程多线程训练。首先将模型完整复制到N个显卡，然后把输入数据分为N份，在各个显卡上进行前向传播。然后，各个显卡把输出发送给主GPU，主GPU计算总的loss，并把损失广播给各个gpu，各个gpu计算梯度，并把各自计算的梯度gather到主gpu，gpu通过梯度进行参数更新，并把新的参数广播到各个gpu。如果模型总的参数量是$\\tau$的话，那么主显卡的输入输出通讯录是$(N-1)\\tau$, 而其他显卡的通信量是$\\tau$\n",
    "\n",
    "### 分布式数据并行(distributed data parallelism)\n",
    "DDP属于真正的多进程训练，每个进程都有独立的优化器与python解释器，并计算梯度。同时，通过集群通信的ring-all-reduce。DDP的多卡负载更加均衡。在ring-all-reduce环节中，所有显卡的通信量都是$2\\tau$\n",
    "\n",
    "### DP与DDP的区别\n",
    "+ DP只适合单机多卡的场景，DDP适合单机多卡、多机多卡。由于DDP属于多进程实现，因此可以避免python解释器的GIL锁带来的影响。\n",
    "+ 由于DP的梯度计算和梯度更新都在主卡上进行，因此主卡负载明显高于其他卡。而DDP的负载相对均衡，训练更高效，这使得DP被弃用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d2923",
   "metadata": {},
   "source": [
    "## Zero零冗余优化器\n",
    "zero技术是微软提出用于消除训练中显存冗余的一种算法，有三个版本，分别为os、os+g、os+g+p。在传统数据并行混合精度训练中，每个gpu中不仅要存储fp16的模型参数、fp16的梯度，以及优化器中fp32的参数、梯度、adam一阶momentum、二阶varianace。如果模型参数是$\\tau$,一个fp16占2bytes，fp32占4bytes，那么显存总占用为$2\\tau+2\\tau+4\\tau+4\\tau+4\\tau+4\\tau=20\\tau$。其中优化器状态占用非常大，且各个gpu的优化器状态是冗余的。  \n",
    "\n",
    "### 普通的分布式训练过程如下：   \n",
    "下图为普通的混合精度分布式训练运行过程：  \n",
    "![](Zero_0.png)  \n",
    "1. 将数据并行，分别传给gpu0、1、2\n",
    "2. 随后根据fp16的参数，进行前向传播与反向传播，计算得到fp16的梯度\n",
    "3. 将梯度精度放大到fp32，并根据此计算fp32的一阶动量、二阶方差，并更新fp32的模型参数\n",
    "4. 将fp32的模型参数精度缩小到fp16并更新参数。\n",
    "\n",
    "### zero_1优化器的训练过程如下：    \n",
    "下图为使用zero os优化器进行混合精度分布式训练的过程：\n",
    "![](Zero_1.png)  \n",
    "1. 将数据并行，分别传给gpu0、1、2\n",
    "2. 随后根据fp16的参数，进行前向传播\n",
    "3. gpu0、1、2总数为3，他们各负责更新$\\tau/3$的参数, 因此，他们也只需要保留三分之一的adam优化器状态在显存中就行了。  \n",
    "4. 三个gpu分别进行反向传播，通过all-reduce等方法，同步三个gpu计算的平均值。\n",
    "5. 三个gpu分别把子集对应部分的梯度传给分片的adam优化器，更新fp16参数, 并将fp16参数进行广播到其他cpu\n",
    "6. 这只方法的通讯量与DDP相同，都是$2\\tau$, 但是显存占用率远低于标准DDP。 \n",
    "\n",
    "### zero_2优化器的训练过程如下：\n",
    "zero os+g在os的基础上，对fp16的梯度也进行了划分，如下：  \n",
    "![](Zero_2.png)  \n",
    "1. 在os+g上，每个gpu只保留所需维护参数部分的fp16梯度\n",
    "2. 在反向传播时，如果该gpu不需要维护这段参数，那么计算完梯度后立即与所有gpu计算的梯度平均，并释放这段梯度。而需要维护这段参数的gpu，则保留这段梯度，用于参数更新。\n",
    "\n",
    "### zero_3：\n",
    "zero os+g+p，甚至把参数也划分了。  \n",
    "![](Zero_3.png)  \n",
    "1. 在前向传播时，各个gpu只保留了需要维护参数段的参数，因此，存有参数的gpu需要把该段的参数广播给其他gpu，进行前向传播，传播结束之后立即释放参数显存。\n",
    "2. 反向传播也需要参数，因此同样持有该参数的gpu需要在反向传播时广播参数，不持有参数的gpu通过参数计算得到梯度之后，立即释放参数显存，并计算梯度均值。\n",
    "3. 由于zero3前向传播和后向传播都需要进行广播参数，因此通信量是大于zero1和2的，显存占用却是最低。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
