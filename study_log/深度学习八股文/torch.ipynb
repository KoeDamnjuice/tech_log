{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f7d149-98e7-4bb2-a085-767997fc0ec8",
   "metadata": {},
   "source": [
    "# Tensor是什么\n",
    "torch中的tensor类似numpy的ndarrays, 是最基本的数据结构，用于存储和操作多维数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6cc53-abe1-4d89-bf5c-e578affa3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e2f0e",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "### 创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d0068-e29c-4138-ab99-a3c621943b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列表到张量\n",
    "tensor_from_list = torch.tensor([1,2,30])\n",
    "# ndarray 搭配张量\n",
    "np_array = np.array([4,5,6])\n",
    "tensor_from_numpy = torch.tensor(np_array)\n",
    "print(tensor_from_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be553003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全零\n",
    "zero_tensor = torch.zeros((3, 4))\n",
    "print(zero_tensor)\n",
    "# 全1\n",
    "ones_tensor = torch.ones((2,2))\n",
    "print(ones_tensor)\n",
    "# 指定范围张量\n",
    "range_tensor = torch.arange(0, 10, 2)\n",
    "print(range_tensor)\n",
    "# 均匀分布的tensor\n",
    "mean_tensor = torch.rand((3,3))\n",
    "print(mean_tensor)\n",
    "# 正态分布tensor\n",
    "normal_tensor = torch.randn((3,3))\n",
    "print(normal_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未初始化的tensor\n",
    "empty_tensor = torch.empty((2,2))\n",
    "\n",
    "# 常见与某个tensor形状相同的tensor\n",
    "same_shape_tensor = torch.ones_like(normal_tensor)\n",
    "print(empty_tensor)\n",
    "print(same_shape_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f5339",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "## 张量的基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量的尺寸\n",
    "# tensor_from_list = torch.tensor([1,2,30])\n",
    "shape_tensor = tensor_from_list.shape\n",
    "print(shape_tensor)\n",
    "\n",
    "# 索引\n",
    "element = tensor_from_list[0]\n",
    "print(element)\n",
    "\n",
    "# 切片\n",
    "sliced_tensor = tensor_from_list[:2]\n",
    "print(sliced_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e34067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改变张量的形状\n",
    "reshaped_tensor = tensor_from_list.view(1,3)\n",
    "# 改成1，3相当于从一维张量转为了二维张量\n",
    "print(reshaped_tensor)\n",
    "# 张量转置\n",
    "transposed_tensor = tensor_from_list.t()\n",
    "print(transposed_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2809a6",
   "metadata": {},
   "source": [
    "**数学计算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d432cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_o = torch.ones(1,3)\n",
    "tensor_rand = torch.randn(1,3)\n",
    "print(tensor_o, tensor_rand)\n",
    "\n",
    "#加法操作\n",
    "tensor_sum = tensor_o + tensor_rand\n",
    "print(\"sum:\", tensor_sum)\n",
    "\n",
    "# 乘法操作: 注意matmul是典型的矩阵乘法\n",
    "product_tensor = torch.matmul(tensor_sum, tensor_o.t())\n",
    "print(product_tensor)\n",
    "product_tensor_2 = torch.matmul(tensor_sum.t(), tensor_o)\n",
    "print(product_tensor_2)\n",
    "\n",
    "# 广播计算\n",
    "tensor_o_broad = tensor_o * 3\n",
    "print(tensor_o_broad)\n",
    "tensor_o_broad_2 = tensor_o_broad + 2\n",
    "print(tensor_o_broad_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e4315f",
   "metadata": {},
   "source": [
    "## 自动求导\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3770440",
   "metadata": {},
   "source": [
    "# 自动求导\n",
    "pytorch自动求导的核心是torch.autograd模块，它为对张量的所有操作提供了自动求导服务，其核心部分包含:\n",
    "+ tensor\n",
    "+ 计算图\n",
    "+ 梯度计算\n",
    "+ 梯度累计\n",
    "+ 离散跟踪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075be503",
   "metadata": {},
   "source": [
    "如果张量的requires_grad属性被设置为True，那么对它的每一步操作都将被跟踪。当结束计算后，可以调用**backward()**方法，来自动计算所有参数的梯度，注意这个梯度是**累加的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 3, requires_grad = True)\n",
    "print(x.grad_fn)\n",
    "# 由于x是用户创建的叶子节点，因此没有跟踪计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3089b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,2, requires_grad= True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f247a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对张量进行一次计算\n",
    "y = x ** 2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "# more\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c69ff9",
   "metadata": {},
   "source": [
    "由于y是对x进行操作的结果，因此这个操作会被跟踪，y的grad_fn属性为grad_fn=<PowBackward0>, 字面上看是幂计算反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad属性默认是false\n",
    "a = torch.randn(3,2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "# .requires_grad_()方法可以原地修改张量的requires_grad属性\n",
    "a.requires_grad_(True)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b897b",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对out反向传播\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78609c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看看d(out)/dx\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061cb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 雅可比向量积的例子:\n",
    "j = torch.randn(3, requires_grad=True)\n",
    "print(j)\n",
    "\n",
    "k = j * 2\n",
    "m = 0\n",
    "while k.data.norm() < 1000:\n",
    "    k = k * 2\n",
    "    m = m + 1\n",
    "print(k)\n",
    "print(m)\n",
    "# 此时k已经不再是标量，autograd不能直接计算梯度，因此只可以求雅可比向量积。\n",
    "v = torch.tensor([0.1, 1.0, 0.0001],dtype=torch.float)\n",
    "k.backward(v)\n",
    "print(j.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c80a0",
   "metadata": {},
   "source": [
    "注意，auto_grad只能直接求**标量**结果的反向传播结果。如果结果是向量的话，对所有输入求导就会得到一个**雅可比矩阵**，而不是该输入的梯度值。因此，如果最终结果是向量的话，backward()需要引入一个与输出同维度的权重向量。\n",
    "\n",
    "___\n",
    "\n",
    "***梯度的累加机制***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果不手动对参数的梯度清零的话，其梯度会一直累加\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859f509",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 离散追踪\n",
    "推理或者验证模型的时候，我们不希望数值计算过程被追踪，因为那样会消耗额外的内存与算力。此时使用torch.no_grad()来临时将tensor的requires_grad属性设置为False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c48a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(j.requires_grad)\n",
    "print((j ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((j ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91dc0e",
   "metadata": {},
   "source": [
    "___\n",
    "tip:如果我们希望改变一个tensor的值，却不想该百年它的autograd记录，那么就直接对tensor.data属性进行操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "\n",
    "print(x.data)  # 其实还是一个tensor\n",
    "print(x.data.requires_grad)\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "y.backward()\n",
    "print(x)\n",
    "print(x.grad)  # 发现导数为2， 与100无关"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3309e0",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## grad_fn属性详解\n",
    "\n",
    "当我们对张量进行运算的时候，得到的张量的grad_fn会自动记录该操作，**并指向一个Function对象**，该对象复制前向传播和反向传播。tensor与Function共同组成一个无环有向的计算图。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993d5dd",
   "metadata": {},
   "source": [
    "# 多gpu并行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296bd25",
   "metadata": {},
   "source": [
    "### 什么是CUDA?\n",
    "CUDA是英伟达GPU的并行运算框架。对GPU的编程是使用CUDA语言完成的。然而pytorch的cuda意思是我们即将使用GPU来处理数据和模型。\n",
    "\n",
    "当我们希望把模型或者数据从cpu迁移到gpu时，就需要使用.cuda()方法（默认0号gpu）。\n",
    "tips：\n",
    "+ pytorch目前不支持amd的opengl接口\n",
    "+ 避免频繁地将数据在cpu和gpu之间切换\n",
    "+ 进行简单操作时，使用cpu\n",
    "\n",
    "——————\n",
    "\n",
    "如何设置默认显卡？两种方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a2c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #设置在文件最开始部分\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICE\"] = \"2\" # 设置默认的显卡\n",
    "# 或者\n",
    "CUDA_VISBLE_DEVICE=0,1 python train.py # 使用0，1两块GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc78cb",
   "metadata": {},
   "source": [
    "## 常见的并行训练方法：\n",
    "\n",
    "**1.mambaout,模型拆分**  \n",
    "![模型拆分图解](https://datawhalechina.github.io/thorough-pytorch/_images/model_parllel.png)  \n",
    "该方法难在gpu之间的通信\n",
    "\n",
    "**2.同一层任务分不到不同数据中**\n",
    "这种方法将同一层模型做拆分，不太理解  \n",
    "\n",
    "**3.不同数据拆分到不同设备(数据并行方式, data parallelism)**  \n",
    "![](https://datawhalechina.github.io/thorough-pytorch/_images/data_parllel.png)  \n",
    "该方法的逻辑是将相同的模型复制到各个显卡中，然后将数据切分，让各个显卡训练数据的一部分，最后进行汇总反向传播。这样可以解决通信问题。\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "### 多卡训练 \n",
    "pytorch提供了data parallel(DP)方式和DistributedDataParallel(DDP)两种多卡训练方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9c48c",
   "metadata": {},
   "source": [
    "***使用CUDA加速训练***  \n",
    "**单卡训练**  \n",
    "非常简单，只要将模型和数据.cuda()就行了  \n",
    "首先要手动指定对程序可见的gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7d547",
   "metadata": {},
   "source": [
    "**多卡DP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5abc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.cuda() # 模型显示转移到CUDA上\n",
    "\n",
    "if torch.cuda.device_count() > 1: # 含有多张GPU的卡\n",
    "\tmodel = nn.DataParallel(model) # 单机多卡DP训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eccc04f",
   "metadata": {},
   "source": [
    "DP的抽象过程，数据被分为多个子集后，分别在各个gpu计算梯度，最后把梯度汇总到一个主gpu上进行参数的更新，这样会使得主gpu的工作负担明显大于其他gpu，成为性能瓶颈。此外，DP只适合单机模式。**注意DP是单进程多线程**\n",
    "\n",
    "问题：既然python解释器有GIL锁，为什么限制不了DP的加速？\n",
    "因为pytorch数据运行于gpu上，由CUDA和其他GPU加速库管理，不受python解释器管理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cd18b",
   "metadata": {},
   "source": [
    "**多机多卡DDP**  \n",
    "DDP是一种多卡多进程方式，每个进程独立运行在一个gpu上，每个gpu独立更新参数，使用高效的通信机制进行梯度信息同步。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c76e7f",
   "metadata": {},
   "source": [
    "## GIL锁对于DP与DDP的影响？\n",
    "由于模型训练大部分内容是在gpu上运行的，因此对于DP，GIL锁对工作的大部分内容几乎没有影响。\n",
    "但是对于**运行于cpu的数据预处理加载和IO操作，还有数据后处理和日志记录等**，可能成为性能瓶颈。\n",
    "但是对于多进程的DDP，由于各个进程都有独立的python解释器，实现了真正的并行运行，因此不受GIL影响。\n",
    "\n",
    "这对于严重依赖 Python runtime 的 models 而言，比如说包含 RNN 层或大量小组件的 models 而言，这尤为重要。什么是python runtime？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611462c7",
   "metadata": {},
   "source": [
    "# 深度学习训练的整体流程\n",
    "+ 数据预处理： 数据格式统一、异常数据清除、数据变换\n",
    "+ 划分训练集、验证集、测试集（可以使用sklearn自带的test_train_split函数）\n",
    "+ 模型选择、损失函数、优化方法\n",
    "+ 超参数设置\n",
    "+ 使用模型拟合训练数据集，在验证集、测试机上评估模型表现\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d39b21",
   "metadata": {},
   "source": [
    "## 包的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81683703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显式指定gpu\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1' #指明gpu为01\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available else cpu)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cce447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一些超参数\n",
    "batch_size = 16\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "epoch = 100\n",
    "# 当然我们可以选择把超参数存储在yaml里面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de9bf5",
   "metadata": {},
   "source": [
    "## 数据读入\n",
    "+ dataset定义数据格式和数据变形形式\n",
    "+ dataloader以迭代的方式，向模型输入批次数据\n",
    "\n",
    "___\n",
    "\n",
    "## dataset\n",
    "**使用torch自建的dataset ImageFolder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以cifar10数据集构建Dataset类的方式\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "# 这里使用了PyTorch自带的ImageFolder类的用于读取按一定结构存储的图片数据（path对应图片存放的目录，目录下包含若干子目录，每个子目录对应属于同一个类的图片）\n",
    "train_data = datasets.ImageFolder(train_path, transfor=data_transform)\n",
    "val_data = datasets.ImageFolder(val_path, transform=data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844a97b",
   "metadata": {},
   "source": [
    "**使用自定义的dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    # 注意实现Dataset父类\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        image1.jpg, 0\n",
    "        image2.jpg, 1\n",
    "        ......\n",
    "        image9.jpg, 9\n",
    "        \n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # 数据加载\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24283aca",
   "metadata": {},
   "source": [
    "构建完成dataset以后，就可以使用dataloader批次读入数据了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "# 参数解析\n",
    "# num_workers, cpu读取数据的进程数\n",
    "# drop_last, 对于最后达不到batch_size数量的样本，是否丢弃？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383a8e8",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 模型构建\n",
    "**Module 类**是 torch.nn 模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374e7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用nn.Module构建多层感知机\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # 在__init__中构建神经网络\n",
    "    def __init__(self, **kwargs):\n",
    "    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 10)\n",
    "    \n",
    "    # 定义模型的前向计算\n",
    "    def forward(self, x):\n",
    "        o = self.act(self.hidden(x))\n",
    "        return self.output(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba250603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1897, -0.2245, -0.0114, -0.3305,  0.2267, -0.1157,  0.2367, -0.0760,\n",
       "          0.0160,  0.0921],\n",
       "        [-0.0816, -0.0649,  0.1001, -0.0160,  0.1412, -0.2122, -0.3227, -0.3230,\n",
       "         -0.2738, -0.1120]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机设置一个输入\n",
    "x = torch.randn(2, 784) # 第一维是batch\n",
    "net = MLP()\n",
    "print(net)\n",
    "net(x)  # 前向计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00db050",
   "metadata": {},
   "source": [
    "### 深度学习有各种各样的层, 本节学习使用torch构建自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96de4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不含参数的层\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82aeaee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (1): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (2): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (3): Parameter containing: [torch.float32 of size 4x1]\n",
      "  )\n",
      ")\n",
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 自定义参数的层，注意Parameter是Tensor的子类型，会被自动添加到模型的参数列表之中,使之可以训练\n",
    "# 这样的话，似乎就可以实现模型手撕了\n",
    "class MyListDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyListDense, self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.params)):\n",
    "            # mm，仅仅可以对付二维矩阵乘法\n",
    "            x = torch.mm(x, self.params[i])\n",
    "        return x\n",
    "net = MyListDense()\n",
    "print(net)\n",
    "\n",
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'linear1': nn.Parameter(torch.randn(4, 4)),\n",
    "                'linear2': nn.Parameter(torch.randn(4, 1))\n",
    "        })\n",
    "        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增\n",
    "\n",
    "    def forward(self, x, choice='linear1'):\n",
    "        return torch.mm(x, self.params[choice])\n",
    "\n",
    "net = MyDictDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaae9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手撕二维卷积！！！\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    print(h,w)\n",
    "    X, K = X.float(), K.float()\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init()\n",
    "        # super 方法用于为子类调用父类方法，type参数\n",
    "        # 传入类型， obj传入实例，在类定义中, obj传入\n",
    "        # self\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "487c328a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 使用padding进行same卷积\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def comp_conv2d(conv2d, X):\n",
    "    X = X.view((1,1) + X.shape) # 为X加入批次和通道维度\n",
    "    Y = conv2d(X)\n",
    "    return Y.view(Y.shape[2:]) # 滤除批次和通道维度\n",
    "\n",
    "# 对输入左右padding两行\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding= 1)\n",
    "\n",
    "X = torch.rand(8,8)\n",
    "# 注意卷积之后前后变换公式: y = (x - h + 2 * p)/s + 1, 注意向下取整\n",
    "print(comp_conv2d(conv2d, X).shape)\n",
    "\n",
    "# 使用高宽不一致的卷积核\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 3), padding=(2, 1))\n",
    "print(comp_conv2d(conv2d, X).shape)\n",
    "\n",
    "# 引入stride\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 5), padding= (0, 1), stride = (3, 4))\n",
    "print(comp_conv2d(conv2d, X).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5e3ba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 6.],\n",
      "        [8., 9.]])\n",
      "tensor([[2.5000, 3.5000],\n",
      "        [6.0000, 7.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 手撕池化层\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def pool2d(X, pool_size, mode = 'max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y\n",
    "\n",
    "X = torch.tensor([[0,1,2],[4,5,6],[7,8,9]], dtype = torch.float)\n",
    "print(pool2d(X, (2,2)))\n",
    "print(pool2d(X,(2,2), \"avg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c7d35",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# 模型初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa4df6",
   "metadata": {},
   "source": [
    "### 为什么参数初始化不能全部为0  \n",
    "当网络中所有权重都初始化为0时，对于任何隐藏层，所有神经元在前向传播和反向传播过程中的行为都会完全相同。这意味着每一层的每个神经元将计算出相同的输出，并且在反向传播中也会接收到相同的梯度。结果是，即使网络的参数被更新，每个权重也会保持相同，这使得隐藏层的多个神经元没有区分开来，因此无法学习到多样化的特征或模式。  \n",
    "此外，对于一些过零的激活函数（如Sigmoid或Tanh），全部初始化为0也会导致梯度消失问题，当激活函数的输入接近0的时候，其反向传播获得的梯度也会接近0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e80c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
