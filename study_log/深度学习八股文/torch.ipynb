{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f7d149-98e7-4bb2-a085-767997fc0ec8",
   "metadata": {},
   "source": [
    "# Tensor是什么\n",
    "torch中的tensor类似numpy的ndarrays, 是最基本的数据结构，用于存储和操作多维数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6b6cc53-abe1-4d89-bf5c-e578affa3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e2f0e",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "### 创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b38d0068-e29c-4138-ab99-a3c621943b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 列表到张量\n",
    "tensor_from_list = torch.tensor([1,2,30])\n",
    "# ndarray 搭配张量\n",
    "np_array = np.array([4,5,6])\n",
    "tensor_from_numpy = torch.tensor(np_array)\n",
    "print(tensor_from_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be553003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([[0.4749, 0.3585, 0.0671],\n",
      "        [0.4649, 0.5255, 0.5668],\n",
      "        [0.1080, 0.7678, 0.1878]])\n",
      "tensor([[ 0.3145,  0.0897,  0.7994],\n",
      "        [ 1.1631, -0.3329,  1.0968],\n",
      "        [ 0.3863, -0.2104,  1.1247]])\n"
     ]
    }
   ],
   "source": [
    "# 全零\n",
    "zero_tensor = torch.zeros((3, 4))\n",
    "print(zero_tensor)\n",
    "# 全1\n",
    "ones_tensor = torch.ones((2,2))\n",
    "print(ones_tensor)\n",
    "# 指定范围张量\n",
    "range_tensor = torch.arange(0, 10, 2)\n",
    "print(range_tensor)\n",
    "# 均匀分布的tensor\n",
    "mean_tensor = torch.rand((3,3))\n",
    "print(mean_tensor)\n",
    "# 正态分布tensor\n",
    "normal_tensor = torch.randn((3,3))\n",
    "print(normal_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3fc02f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.3694e-38, 0.0000e+00],\n",
      "        [3.0000e+00, 3.0000e+00]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 未初始化的tensor\n",
    "empty_tensor = torch.empty((2,2))\n",
    "\n",
    "# 常见与某个tensor形状相同的tensor\n",
    "same_shape_tensor = torch.ones_like(normal_tensor)\n",
    "print(empty_tensor)\n",
    "print(same_shape_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f5339",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "## 张量的基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39f4081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor(1)\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# 张量的尺寸\n",
    "# tensor_from_list = torch.tensor([1,2,30])\n",
    "shape_tensor = tensor_from_list.shape\n",
    "print(shape_tensor)\n",
    "\n",
    "# 索引\n",
    "element = tensor_from_list[0]\n",
    "print(element)\n",
    "\n",
    "# 切片\n",
    "sliced_tensor = tensor_from_list[:2]\n",
    "print(sliced_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29e34067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2, 30]])\n",
      "tensor([ 1,  2, 30])\n"
     ]
    }
   ],
   "source": [
    "# 改变张量的形状\n",
    "reshaped_tensor = tensor_from_list.view(1,3)\n",
    "# 改成1，3相当于从一维张量转为了二维张量\n",
    "print(reshaped_tensor)\n",
    "# 张量转置\n",
    "transposed_tensor = tensor_from_list.t()\n",
    "print(transposed_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2809a6",
   "metadata": {},
   "source": [
    "**数学计算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d432cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.]]) tensor([[-0.4956, -1.4897,  1.7172]])\n",
      "sum: tensor([[ 0.5044, -0.4897,  2.7172]])\n",
      "tensor([[2.7319]])\n",
      "tensor([[ 0.5044,  0.5044,  0.5044],\n",
      "        [-0.4897, -0.4897, -0.4897],\n",
      "        [ 2.7172,  2.7172,  2.7172]])\n",
      "tensor([[3., 3., 3.]])\n",
      "tensor([[5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "tensor_o = torch.ones(1,3)\n",
    "tensor_rand = torch.randn(1,3)\n",
    "print(tensor_o, tensor_rand)\n",
    "\n",
    "#加法操作\n",
    "tensor_sum = tensor_o + tensor_rand\n",
    "print(\"sum:\", tensor_sum)\n",
    "\n",
    "# 乘法操作: 注意matmul是典型的矩阵乘法\n",
    "product_tensor = torch.matmul(tensor_sum, tensor_o.t())\n",
    "print(product_tensor)\n",
    "product_tensor_2 = torch.matmul(tensor_sum.t(), tensor_o)\n",
    "print(product_tensor_2)\n",
    "\n",
    "# 广播计算\n",
    "tensor_o_broad = tensor_o * 3\n",
    "print(tensor_o_broad)\n",
    "tensor_o_broad_2 = tensor_o_broad + 2\n",
    "print(tensor_o_broad_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e4315f",
   "metadata": {},
   "source": [
    "## 自动求导\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3770440",
   "metadata": {},
   "source": [
    "# 自动求导\n",
    "pytorch自动求导的核心是torch.autograd模块，它为对张量的所有操作提供了自动求导服务，其核心部分包含:\n",
    "+ tensor\n",
    "+ 计算图\n",
    "+ 梯度计算\n",
    "+ 梯度累计\n",
    "+ 离散跟踪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075be503",
   "metadata": {},
   "source": [
    "如果张量的requires_grad属性被设置为True，那么对它的每一步操作都将被跟踪。当结束计算后，可以调用**backward()**方法，来自动计算所有参数的梯度，注意这个梯度是**累加的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5ef5f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3, requires_grad = True)\n",
    "print(x.grad_fn)\n",
    "# 由于x是用户创建的叶子节点，因此没有跟踪计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d3089b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad= True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "73f247a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x000001DC707B0AC0>\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<MulBackward0>) tensor(3., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 对张量进行一次计算\n",
    "y = x ** 2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "# more\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c69ff9",
   "metadata": {},
   "source": [
    "由于y是对x进行操作的结果，因此这个操作会被跟踪，y的grad_fn属性为grad_fn=<PowBackward0>, 字面上看是幂计算反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "35bd9702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "<SumBackward0 object at 0x000001D8C06C91F0>\n"
     ]
    }
   ],
   "source": [
    "# requires_grad属性默认是false\n",
    "a = torch.randn(3,2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "# .requires_grad_()方法可以原地修改张量的requires_grad属性\n",
    "a.requires_grad_(True)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b897b",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d3e564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对out反向传播\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "78609c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 看看d(out)/dx\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "061cb172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3160,  0.9796, -2.0975], requires_grad=True)\n",
      "tensor([ -161.7708,   501.5692, -1073.9320], grad_fn=<MulBackward0>)\n",
      "8\n",
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "# 雅可比向量积的例子:\n",
    "j = torch.randn(3, requires_grad=True)\n",
    "print(j)\n",
    "\n",
    "k = j * 2\n",
    "m = 0\n",
    "while k.data.norm() < 1000:\n",
    "    k = k * 2\n",
    "    m = m + 1\n",
    "print(k)\n",
    "print(m)\n",
    "# 此时k已经不再是标量，autograd不能直接计算梯度，因此只可以求雅可比向量积。\n",
    "v = torch.tensor([0.1, 1.0, 0.0001],dtype=torch.float)\n",
    "k.backward(v)\n",
    "print(j.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c80a0",
   "metadata": {},
   "source": [
    "注意，auto_grad只能直接求**标量**结果的反向传播结果。如果结果是向量的话，对所有输入求导就会得到一个**雅可比矩阵**，而不是该输入的梯度值。因此，如果最终结果是向量的话，backward()需要引入一个与输出同维度的权重向量。\n",
    "\n",
    "___\n",
    "\n",
    "***梯度的累加机制***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0a73a4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 如果不手动对参数的梯度清零的话，其梯度会一直累加\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859f509",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 离散追踪\n",
    "推理或者验证模型的时候，我们不希望数值计算过程被追踪，因为那样会消耗额外的内存与算力。此时使用torch.no_grad()来临时将tensor的requires_grad属性设置为False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c6c48a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(j.requires_grad)\n",
    "print((j ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((j ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91dc0e",
   "metadata": {},
   "source": [
    "___\n",
    "tip:如果我们希望改变一个tensor的值，却不想该百年它的autograd记录，那么就直接对tensor.data属性进行操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e1a4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "\n",
    "print(x.data)  # 其实还是一个tensor\n",
    "print(x.data.requires_grad)\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "y.backward()\n",
    "print(x)\n",
    "print(x.grad)  # 发现导数为2， 与100无关"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3309e0",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## grad_fn属性详解\n",
    "\n",
    "当我们对张量进行运算的时候，得到的张量的grad_fn会自动记录该操作，**并指向一个Function对象**，该对象复制前向传播和反向传播。tensor与Function共同组成一个无环有向的计算图。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993d5dd",
   "metadata": {},
   "source": [
    "# 多gpu并行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296bd25",
   "metadata": {},
   "source": [
    "### 什么是CUDA?\n",
    "CUDA是英伟达GPU的并行运算框架。对GPU的编程是使用CUDA语言完成的。然而pytorch的cuda意思是我们即将使用GPU来处理数据和模型。\n",
    "\n",
    "当我们希望把模型或者数据从cpu迁移到gpu时，就需要使用.cuda()方法（默认0号gpu）。\n",
    "tips：\n",
    "+ pytorch目前不支持amd的opengl接口\n",
    "+ 避免频繁地将数据在cpu和gpu之间切换\n",
    "+ 进行简单操作时，使用cpu\n",
    "\n",
    "——————\n",
    "\n",
    "如何设置默认显卡？两种方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a2c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #设置在文件最开始部分\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICE\"] = \"2\" # 设置默认的显卡\n",
    "# 或者\n",
    "CUDA_VISBLE_DEVICE=0,1 python train.py # 使用0，1两块GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc78cb",
   "metadata": {},
   "source": [
    "## 常见的并行训练方法：\n",
    "\n",
    "**1.mambaout,模型拆分**  \n",
    "![模型拆分图解](https://datawhalechina.github.io/thorough-pytorch/_images/model_parllel.png)  \n",
    "该方法难在gpu之间的通信\n",
    "\n",
    "**2.同一层任务分不到不同数据中**\n",
    "这种方法将同一层模型做拆分，不太理解  \n",
    "\n",
    "**3.不同数据拆分到不同设备(数据并行方式, data parallelism)**  \n",
    "![](https://datawhalechina.github.io/thorough-pytorch/_images/data_parllel.png)  \n",
    "该方法的逻辑是将相同的模型复制到各个显卡中，然后将数据切分，让各个显卡训练数据的一部分，最后进行汇总反向传播。这样可以解决通信问题。\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "### 多卡训练 \n",
    "pytorch提供了data parallel(DP)方式和DistributedDataParallel(DDP)两种多卡训练方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9c48c",
   "metadata": {},
   "source": [
    "***使用CUDA加速训练***  \n",
    "**单卡训练**  \n",
    "非常简单，只要将模型和数据.cuda()就行了  \n",
    "首先要手动指定对程序可见的gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7d547",
   "metadata": {},
   "source": [
    "**多卡DP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5abc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.cuda() # 模型显示转移到CUDA上\n",
    "\n",
    "if torch.cuda.device_count() > 1: # 含有多张GPU的卡\n",
    "\tmodel = nn.DataParallel(model) # 单机多卡DP训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eccc04f",
   "metadata": {},
   "source": [
    "DP的抽象过程，数据被分为多个子集后，分别在各个gpu计算梯度，最后把梯度汇总到一个主gpu上进行参数的更新，这样会使得主gpu的工作负担明显大于其他gpu，成为性能瓶颈。此外，DP只适合单机模式。**注意DP是单进程多线程**\n",
    "\n",
    "问题：既然python解释器有GIL锁，为什么限制不了DP的加速？\n",
    "因为pytorch数据运行于gpu上，由CUDA和其他GPU加速库管理，不受python解释器管理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cd18b",
   "metadata": {},
   "source": [
    "**多机多卡DDP**  \n",
    "DDP是一种多卡多进程方式，每个进程独立运行在一个gpu上，每个gpu独立更新参数，使用高效的通信机制进行梯度信息同步。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c76e7f",
   "metadata": {},
   "source": [
    "## GIL锁对于DP与DDP的影响？\n",
    "由于模型训练大部分内容是在gpu上运行的，因此对于DP，GIL锁对工作的大部分内容几乎没有影响。\n",
    "但是对于**运行于cpu的数据预处理加载和IO操作，还有数据后处理和日志记录等**，可能成为性能瓶颈。\n",
    "但是对于多进程的DDP，由于各个进程都有独立的python解释器，实现了真正的并行运行，因此不受GIL影响。\n",
    "\n",
    "这对于严重依赖 Python runtime 的 models 而言，比如说包含 RNN 层或大量小组件的 models 而言，这尤为重要。什么是python runtime？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611462c7",
   "metadata": {},
   "source": [
    "# 深度学习训练的整体流程\n",
    "+ 数据预处理： 数据格式统一、异常数据清除、数据变换\n",
    "+ 划分训练集、验证集、测试集（可以使用sklearn自带的test_train_split函数）\n",
    "+ 模型选择、损失函数、优化方法\n",
    "+ 超参数设置\n",
    "+ 使用模型拟合训练数据集，在验证集、测试机上评估模型表现\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d39b21",
   "metadata": {},
   "source": [
    "## 包的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81683703",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "File \u001b[1;32mc:\\Users\\ziyu.zhan_sx\\AppData\\Local\\miniconda3\\envs\\coder\\lib\\site-packages\\torch\\__init__.py:562\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    564\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
